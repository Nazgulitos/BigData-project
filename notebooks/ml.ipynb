{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb0ed7a",
   "metadata": {},
   "source": [
    "## Stage 3: Predictive Data Analytics\n",
    "\n",
    "### Imports and Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc12d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/08 16:51:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/08 16:51:45 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/05/08 16:51:45 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Created.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "team = \"team15\"\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(f\"{team} - spark ML\")\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3f496",
   "metadata": {},
   "source": [
    "### Load data from Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c42bdc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|             retake1|\n",
      "|             root_db|\n",
      "|                show|\n",
      "|     team0_projectdb|\n",
      "|    team11_projectdb|\n",
      "|           team12_db|\n",
      "|team12_hive_proje...|\n",
      "|    team12_projectdb|\n",
      "|    team13_projectdb|\n",
      "|    team14_projectdb|\n",
      "|    team15_projectdb|\n",
      "|    team16_projectdb|\n",
      "|    team18_projectdb|\n",
      "|    team19_projectdb|\n",
      "|     team1_projectdb|\n",
      "|    team20_projectdb|\n",
      "| team21_projectdb_v2|\n",
      "| team21_projectdb_v3|\n",
      "| team21_projectdb_v4|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all databases\n",
    "\n",
    "spark.sql(\"SHOW DATABASES;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597ab429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----------+\n",
      "|       namespace|           tableName|isTemporary|\n",
      "+----------------+--------------------+-----------+\n",
      "|team15_projectdb|             airport|      false|\n",
      "|team15_projectdb|   airport_optimized|      false|\n",
      "|team15_projectdb|  cancellationreason|      false|\n",
      "|team15_projectdb|cancellationreaso...|      false|\n",
      "|team15_projectdb|              flight|      false|\n",
      "|team15_projectdb|    flight_optimized|      false|\n",
      "|team15_projectdb|          q1_results|      false|\n",
      "|team15_projectdb|          q2_results|      false|\n",
      "|team15_projectdb|          q3_results|      false|\n",
      "|team15_projectdb|          q4_results|      false|\n",
      "|team15_projectdb|          q5_results|      false|\n",
      "|team15_projectdb|          q6_results|      false|\n",
      "+----------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all tables\n",
    "\n",
    "hive_db_name = f\"{team}_projectdb\"\n",
    "spark.sql(f\"USE {hive_db_name};\")\n",
    "spark.sql(\"SHOW TABLES;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d4d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Hive tables\n",
    "\n",
    "airport = spark.read.format(\"avro\").table(f'{hive_db_name}.airport')\n",
    "flight = spark.read.format(\"avro\").table(f'{hive_db_name}.flight')\n",
    "cancellationreason = spark.read.format(\"avro\").table(f'{hive_db_name}.cancellationreason')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f589b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- flightid: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- dayofmonth: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- deptime: double (nullable = true)\n",
      " |-- crsdeptime: double (nullable = true)\n",
      " |-- arrtime: double (nullable = true)\n",
      " |-- crsarrtime: double (nullable = true)\n",
      " |-- actualelapsedtime: double (nullable = true)\n",
      " |-- crselapsedtime: double (nullable = true)\n",
      " |-- airtime: double (nullable = true)\n",
      " |-- arrdelay: double (nullable = true)\n",
      " |-- depdelay: double (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- taxiin: double (nullable = true)\n",
      " |-- taxiout: double (nullable = true)\n",
      " |-- cancelled: double (nullable = true)\n",
      " |-- cancellationcode: string (nullable = true)\n",
      " |-- diverted: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run some queries\n",
    "\n",
    "airport.printSchema()\n",
    "flight.printSchema()\n",
    "cancellationreason.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72ccfa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 16:52:06 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------------+\n",
      "|flightid|cancelled|cancellationcode|\n",
      "+--------+---------+----------------+\n",
      "| 6617976|      0.0|            NULL|\n",
      "| 6617977|      0.0|            NULL|\n",
      "| 6617978|      0.0|            NULL|\n",
      "| 6617979|      0.0|            NULL|\n",
      "| 6617980|      0.0|            NULL|\n",
      "| 6617981|      0.0|            NULL|\n",
      "| 6617982|      0.0|            NULL|\n",
      "| 6617983|      0.0|            NULL|\n",
      "| 6617984|      0.0|            NULL|\n",
      "| 6617985|      0.0|            NULL|\n",
      "+--------+---------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT flightid, cancelled, cancellationcode FROM flight LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f65c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|code|\n",
      "+----+\n",
      "| ABE|\n",
      "| ABI|\n",
      "| ABQ|\n",
      "| ABR|\n",
      "| ABY|\n",
      "| ACK|\n",
      "| ACT|\n",
      "| ACV|\n",
      "| ACY|\n",
      "| ADK|\n",
      "+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM airport LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c32d7313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|code|description|\n",
      "+----+-----------+\n",
      "|   A|    Carrier|\n",
      "|   B|    Weather|\n",
      "|   C|        NAS|\n",
      "|   D|   Security|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM cancellationreason LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce77715",
   "metadata": {},
   "source": [
    "### Data prep for ML modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7648ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, sin, cos, pi, hour, minute, dayofmonth, dayofweek, year as f_year\n",
    "from pyspark.sql.types import IntegerType, StringType, DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ccb70af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- flightid: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- dayofmonth: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- deptime: double (nullable = true)\n",
      " |-- crsdeptime: double (nullable = true)\n",
      " |-- arrtime: double (nullable = true)\n",
      " |-- crsarrtime: double (nullable = true)\n",
      " |-- actualelapsedtime: double (nullable = true)\n",
      " |-- crselapsedtime: double (nullable = true)\n",
      " |-- airtime: double (nullable = true)\n",
      " |-- arrdelay: double (nullable = true)\n",
      " |-- depdelay: double (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- taxiin: double (nullable = true)\n",
      " |-- taxiout: double (nullable = true)\n",
      " |-- cancelled: double (nullable = true)\n",
      " |-- cancellationcode: string (nullable = true)\n",
      " |-- diverted: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive_table_name = \"flight\"\n",
    "df = spark.table(f\"{hive_db_name}.{hive_table_name}\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "539e6223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+----------+---------+-------+----------+-------+----------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+\n",
      "|flightid|year|month|dayofmonth|dayofweek|deptime|crsdeptime|arrtime|crsarrtime|actualelapsedtime|crselapsedtime|airtime|arrdelay|depdelay|origin|dest|distance|taxiin|taxiout|cancelled|cancellationcode|diverted|\n",
      "+--------+----+-----+----------+---------+-------+----------+-------+----------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+\n",
      "| 6617976|2017|   11|        12|        7| 2049.0|    2055.0| 2144.0|    2205.0|            115.0|         130.0|   97.0|   -21.0|    -6.0|   DCA| MKE|   634.0|   3.0|   15.0|      0.0|            NULL|     0.0|\n",
      "| 6617977|2017|   11|        12|        7| 1430.0|    1430.0| 1523.0|    1535.0|            113.0|         125.0|  100.0|   -12.0|     0.0|   DCA| MKE|   634.0|   3.0|   10.0|      0.0|            NULL|     0.0|\n",
      "| 6617978|2017|   11|        12|        7| 1308.0|    1305.0| 1448.0|    1500.0|            160.0|         175.0|  142.0|   -12.0|     3.0|   DCA| MSY|   969.0|   5.0|   13.0|      0.0|            NULL|     0.0|\n",
      "| 6617979|2017|   11|        12|        7| 1904.0|    1855.0| 2043.0|    2050.0|            159.0|         175.0|  148.0|    -7.0|     9.0|   DCA| MSY|   969.0|   2.0|    9.0|      0.0|            NULL|     0.0|\n",
      "| 6617980|2017|   11|        12|        7| 1640.0|    1645.0| 1819.0|    1845.0|            159.0|         180.0|  146.0|   -26.0|    -5.0|   DCA| OMA|  1012.0|   3.0|   10.0|      0.0|            NULL|     0.0|\n",
      "| 6617981|2017|   11|        12|        7| 2124.0|    1955.0| 2228.0|    2115.0|             64.0|          80.0|   49.0|    73.0|    89.0|   DCA| PVD|   356.0|   3.0|   12.0|      0.0|            NULL|     0.0|\n",
      "| 6617982|2017|   11|        12|        7| 1137.0|    1145.0| 1242.0|    1305.0|             65.0|          80.0|   48.0|   -23.0|    -8.0|   DCA| PVD|   356.0|   4.0|   13.0|      0.0|            NULL|     0.0|\n",
      "| 6617983|2017|   11|        12|        7|  859.0|     755.0| 1016.0|     915.0|            137.0|         140.0|  115.0|    61.0|    64.0|   DCA| STL|   719.0|   3.0|   19.0|      0.0|            NULL|     0.0|\n",
      "| 6617984|2017|   11|        12|        7| 1154.0|    1155.0| 1309.0|    1320.0|            135.0|         145.0|  114.0|   -11.0|    -1.0|   DCA| STL|   719.0|   3.0|   18.0|      0.0|            NULL|     0.0|\n",
      "| 6617985|2017|   11|        12|        7| 1851.0|    1855.0| 2002.0|    2015.0|            131.0|         140.0|  107.0|   -13.0|    -4.0|   DCA| STL|   719.0|   8.0|   16.0|      0.0|            NULL|     0.0|\n",
      "+--------+----+-----+----------+---------+-------+----------+-------+----------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae985a1a-8cf9-40f2-b0ae-fce22ea7e0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 18505725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08433cfa-b224-45fa-abec-5e585fbce54a",
   "metadata": {},
   "source": [
    "#### Selecting features (drop unnecessary columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "619cf14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flightid', 'year', 'month', 'dayofmonth', 'dayofweek', 'deptime', 'crsdeptime', 'arrtime', 'crsarrtime', 'actualelapsedtime', 'crselapsedtime', 'airtime', 'arrdelay', 'depdelay', 'origin', 'dest', 'distance', 'taxiin', 'taxiout', 'cancelled', 'cancellationcode', 'diverted']\n"
     ]
    }
   ],
   "source": [
    "columns = df.columns\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5748fb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+----------+---------+-------+----------+-------+----------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+\n",
      "|flightid|year|month|dayofmonth|dayofweek|deptime|crsdeptime|arrtime|crsarrtime|actualelapsedtime|crselapsedtime|airtime|arrdelay|depdelay|origin|dest|distance|taxiin|taxiout|cancelled|cancellationcode|diverted|\n",
      "+--------+----+-----+----------+---------+-------+----------+-------+----------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+\n",
      "|       0|   0|    0|         0|        0| 256081|         0| 271763|         0|           309166|            23| 309166|  311764|  261033|     0|   0|       0|271764| 263393|        0|        18240587|       0|\n",
      "+--------+----+-----+----------+---------+-------+----------+-------+----------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "null_counts = df.select([\n",
    "    F.count(F.when(F.col(c).isNull(), c)).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b021ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- dayofmonth: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- crsdeptime: double (nullable = true)\n",
      " |-- crsarrtime: double (nullable = true)\n",
      " |-- crselapsedtime: double (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- cancelled: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    'year', 'month', 'dayofmonth', 'dayofweek',      # timestamps (will be transformed later)\n",
    "    'crsdeptime', 'crsarrtime',                      # timestamps (will be transformed later)\n",
    "    'crselapsedtime',\n",
    "    'origin', 'dest', 'distance',\n",
    "]\n",
    "label = \"cancelled\"\n",
    "\n",
    "df = df.select(features + [label])\n",
    "df = df.withColumn(label, F.col(label).cast(DoubleType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9fba3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+----------+----------+--------------+------+----+--------+---------+\n",
      "|year|month|dayofmonth|dayofweek|crsdeptime|crsarrtime|crselapsedtime|origin|dest|distance|cancelled|\n",
      "+----+-----+----------+---------+----------+----------+--------------+------+----+--------+---------+\n",
      "|2017|   11|        12|        7|    2055.0|    2205.0|         130.0|   DCA| MKE|   634.0|      0.0|\n",
      "|2017|   11|        12|        7|    1430.0|    1535.0|         125.0|   DCA| MKE|   634.0|      0.0|\n",
      "|2017|   11|        12|        7|    1305.0|    1500.0|         175.0|   DCA| MSY|   969.0|      0.0|\n",
      "|2017|   11|        12|        7|    1855.0|    2050.0|         175.0|   DCA| MSY|   969.0|      0.0|\n",
      "|2017|   11|        12|        7|    1645.0|    1845.0|         180.0|   DCA| OMA|  1012.0|      0.0|\n",
      "|2017|   11|        12|        7|    1955.0|    2115.0|          80.0|   DCA| PVD|   356.0|      0.0|\n",
      "|2017|   11|        12|        7|    1145.0|    1305.0|          80.0|   DCA| PVD|   356.0|      0.0|\n",
      "|2017|   11|        12|        7|     755.0|     915.0|         140.0|   DCA| STL|   719.0|      0.0|\n",
      "|2017|   11|        12|        7|    1155.0|    1320.0|         145.0|   DCA| STL|   719.0|      0.0|\n",
      "|2017|   11|        12|        7|    1855.0|    2015.0|         140.0|   DCA| STL|   719.0|      0.0|\n",
      "+----+-----+----------+---------+----------+----------+--------------+------+----+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5df5008f-7dc2-4685-9663-3f85836a7046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|sum(cancelled)|\n",
      "+--------------+\n",
      "|      265138.0|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df.select(sum(\"cancelled\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e37e0-5657-4753-bfba-396156f1d59a",
   "metadata": {},
   "source": [
    "#### Dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfc75fd-6977-4739-9996-bc2d18ea040a",
   "metadata": {},
   "source": [
    "1. Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1b1afb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after NA drop: 18505702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.na.drop()\n",
    "print(f\"Number of rows after NA drop: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b587b300-5921-4c3f-9eaf-edf66f5de5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+----------+----------+--------------+------+----+--------+---------+\n",
      "|year|month|dayofmonth|dayofweek|crsdeptime|crsarrtime|crselapsedtime|origin|dest|distance|cancelled|\n",
      "+----+-----+----------+---------+----------+----------+--------------+------+----+--------+---------+\n",
      "|   0|    0|         0|        0|         0|         0|             0|     0|   0|       0|        0|\n",
      "+----+-----+----------+---------+----------+----------+--------------+------+----+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "null_counts = df.select([\n",
    "    F.count(F.when(F.col(c).isNull(), c)).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9c90c09-e3e9-4319-ac61-fd1f823048db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+----------+----------+--------------+------+----+--------+---------+\n",
      "|year|month|dayofmonth|dayofweek|crsdeptime|crsarrtime|crselapsedtime|origin|dest|distance|cancelled|\n",
      "+----+-----+----------+---------+----------+----------+--------------+------+----+--------+---------+\n",
      "|2017|   11|        12|        7|    2055.0|    2205.0|         130.0|   DCA| MKE|   634.0|      0.0|\n",
      "|2017|   11|        12|        7|    1430.0|    1535.0|         125.0|   DCA| MKE|   634.0|      0.0|\n",
      "|2017|   11|        12|        7|    1305.0|    1500.0|         175.0|   DCA| MSY|   969.0|      0.0|\n",
      "|2017|   11|        12|        7|    1855.0|    2050.0|         175.0|   DCA| MSY|   969.0|      0.0|\n",
      "|2017|   11|        12|        7|    1645.0|    1845.0|         180.0|   DCA| OMA|  1012.0|      0.0|\n",
      "|2017|   11|        12|        7|    1955.0|    2115.0|          80.0|   DCA| PVD|   356.0|      0.0|\n",
      "|2017|   11|        12|        7|    1145.0|    1305.0|          80.0|   DCA| PVD|   356.0|      0.0|\n",
      "|2017|   11|        12|        7|     755.0|     915.0|         140.0|   DCA| STL|   719.0|      0.0|\n",
      "|2017|   11|        12|        7|    1155.0|    1320.0|         145.0|   DCA| STL|   719.0|      0.0|\n",
      "|2017|   11|        12|        7|    1855.0|    2015.0|         140.0|   DCA| STL|   719.0|      0.0|\n",
      "+----+-----+----------+---------+----------+----------+--------------+------+----+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b66a967-8718-463b-967f-1e2e7d0d17d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|sum(cancelled)|\n",
      "+--------------+\n",
      "|      265122.0|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df.select(sum(\"cancelled\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f1042-9519-4274-a807-3dbbc46bc4f0",
   "metadata": {},
   "source": [
    "2. Decompose time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18ce206a-165c-4964-a59b-872ebbe0e306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+----------+----------+--------------+------+----+--------+---------+-------------------+------------------+------------------+-------------------+--------------------+-------------+\n",
      "|year|month|dayofmonth|dayofweek|crsdeptime|crsarrtime|crselapsedtime|origin|dest|distance|cancelled|          month_sin|         month_cos|    dayofmonth_sin|     dayofmonth_cos|       dayofweek_sin|dayofweek_cos|\n",
      "+----+-----+----------+---------+----------+----------+--------------+------+----+--------+---------+-------------------+------------------+------------------+-------------------+--------------------+-------------+\n",
      "|2017|   11|        12|        7|    2055.0|    2205.0|         130.0|   DCA| MKE|   634.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|\n",
      "|2017|   11|        12|        7|    1430.0|    1535.0|         125.0|   DCA| MKE|   634.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|\n",
      "|2017|   11|        12|        7|    1305.0|    1500.0|         175.0|   DCA| MSY|   969.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|\n",
      "|2017|   11|        12|        7|    1855.0|    2050.0|         175.0|   DCA| MSY|   969.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|\n",
      "|2017|   11|        12|        7|    1645.0|    1845.0|         180.0|   DCA| OMA|  1012.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|\n",
      "|2017|   11|        12|        7|    1955.0|    2115.0|          80.0|   DCA| PVD|   356.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|\n",
      "|2017|   11|        12|        7|    1145.0|    1305.0|          80.0|   DCA| PVD|   356.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|\n",
      "|2017|   11|        12|        7|     755.0|     915.0|         140.0|   DCA| STL|   719.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|\n",
      "|2017|   11|        12|        7|    1155.0|    1320.0|         145.0|   DCA| STL|   719.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|\n",
      "|2017|   11|        12|        7|    1855.0|    2015.0|         140.0|   DCA| STL|   719.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|\n",
      "+----+-----+----------+---------+----------+----------+--------------+------+----+--------+---------+-------------------+------------------+------------------+-------------------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df \\\n",
    "    .withColumn(\"month_sin\", F.sin(2*math.pi * F.col(\"month\") / 12.0))\\\n",
    "    .withColumn(\"month_cos\", F.cos(2*math.pi * F.col(\"month\") / 12.0))\\\n",
    "    .withColumn(\"dayofmonth_sin\", F.sin(2*math.pi * F.col(\"dayofmonth\") / 31.0))\\\n",
    "    .withColumn(\"dayofmonth_cos\", F.cos(2*math.pi * F.col(\"dayofmonth\") / 31.0))\\\n",
    "    .withColumn(\"dayofweek_sin\", F.sin(2*math.pi * F.col(\"dayofweek\") / 7.0))\\\n",
    "    .withColumn(\"dayofweek_cos\", F.cos(2*math.pi * F.col(\"dayofweek\") / 7.0))\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd101b54-958a-4d18-9d6c-1dab40c0122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hhmm(colname):\n",
    "    return (\n",
    "        F.floor(F.col(colname) / 100).cast(IntegerType()),\n",
    "        (F.col(colname) % 100).cast(IntegerType())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bab1f1e9-2e60-4eef-9edd-826c7454c697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+----------+----------+--------------+------+----+--------+---------+-------------------+------------------+------------------+-------------------+--------------------+-------------+-------------------+--------------------+--------------------+--------------------+-------------------+--------------------+-------------------+--------------------+\n",
      "|year|month|dayofmonth|dayofweek|crsdeptime|crsarrtime|crselapsedtime|origin|dest|distance|cancelled|          month_sin|         month_cos|    dayofmonth_sin|     dayofmonth_cos|       dayofweek_sin|dayofweek_cos|  crsdeptime_hr_sin|   crsdeptime_hr_cos|   crsdeptime_mn_sin|   crsdeptime_mn_cos|  crsarrtime_hr_sin|   crsarrtime_hr_cos|  crsarrtime_mn_sin|   crsarrtime_mn_cos|\n",
      "+----+-----+----------+---------+----------+----------+--------------+------+----+--------+---------+-------------------+------------------+------------------+-------------------+--------------------+-------------+-------------------+--------------------+--------------------+--------------------+-------------------+--------------------+-------------------+--------------------+\n",
      "|2017|   11|        12|        7|    2055.0|    2205.0|         130.0|   DCA| MKE|   634.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|-0.8660254037844386|  0.5000000000000001|-0.49999999999999967|  0.8660254037844388|-0.5000000000000004|  0.8660254037844384|0.49999999999999994|  0.8660254037844387|\n",
      "|2017|   11|        12|        7|    1430.0|    1535.0|         125.0|   DCA| MKE|   634.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|-0.4999999999999997| -0.8660254037844388|5.66553889764798E-16|                -1.0|-0.7071067811865471| -0.7071067811865479|-0.4999999999999997| -0.8660254037844388|\n",
      "|2017|   11|        12|        7|    1305.0|    1500.0|         175.0|   DCA| MSY|   969.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|-0.2588190451025208| -0.9659258262890683| 0.49999999999999994|  0.8660254037844387|-0.7071067811865471| -0.7071067811865479|                0.0|                 1.0|\n",
      "|2017|   11|        12|        7|    1855.0|    2050.0|         175.0|   DCA| MSY|   969.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|               -1.0|-1.83697019872102...|-0.49999999999999967|  0.8660254037844388|-0.8660254037844386|  0.5000000000000001|-0.8660254037844386|  0.5000000000000001|\n",
      "|2017|   11|        12|        7|    1645.0|    1845.0|         180.0|   DCA| OMA|  1012.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|-0.8660254037844385| -0.5000000000000004|                -1.0|-1.83697019872102...|               -1.0|-1.83697019872102...|               -1.0|-1.83697019872102...|\n",
      "|2017|   11|        12|        7|    1955.0|    2115.0|          80.0|   DCA| PVD|   356.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|-0.9659258262890684|  0.2588190451025203|-0.49999999999999967|  0.8660254037844388|-0.7071067811865477|  0.7071067811865474|                1.0|2.83276944882399E-16|\n",
      "|2017|   11|        12|        7|    1145.0|    1305.0|          80.0|   DCA| PVD|   356.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|  0.258819045102521| -0.9659258262890682|                -1.0|-1.83697019872102...|-0.2588190451025208| -0.9659258262890683|0.49999999999999994|  0.8660254037844387|\n",
      "|2017|   11|        12|        7|     755.0|     915.0|         140.0|   DCA| STL|   719.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0| 0.9659258262890683|-0.25881904510252063|-0.49999999999999967|  0.8660254037844388| 0.7071067811865476| -0.7071067811865475|                1.0|2.83276944882399E-16|\n",
      "|2017|   11|        12|        7|    1155.0|    1320.0|         145.0|   DCA| STL|   719.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|  0.258819045102521| -0.9659258262890682|-0.49999999999999967|  0.8660254037844388|-0.2588190451025208| -0.9659258262890683| 0.8660254037844387| -0.4999999999999998|\n",
      "|2017|   11|        12|        7|    1855.0|    2015.0|         140.0|   DCA| STL|   719.0|      0.0|-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.44929359829470...|          1.0|               -1.0|-1.83697019872102...|-0.49999999999999967|  0.8660254037844388|-0.8660254037844386|  0.5000000000000001|                1.0|2.83276944882399E-16|\n",
      "+----+-----+----------+---------+----------+----------+--------------+------+----+--------+---------+-------------------+------------------+------------------+-------------------+--------------------+-------------+-------------------+--------------------+--------------------+--------------------+-------------------+--------------------+-------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_features = ['crsdeptime', 'crsarrtime']\n",
    "\n",
    "for tf in time_features:\n",
    "    hr, mn = split_hhmm(tf)\n",
    "    df = df \\\n",
    "        .withColumn(f\"{tf}_hr_sin\", F.sin(2*math.pi * hr / 24.0)) \\\n",
    "        .withColumn(f\"{tf}_hr_cos\", F.cos(2*math.pi * hr / 24.0)) \\\n",
    "        .withColumn(f\"{tf}_mn_sin\", F.sin(2*math.pi * mn / 60.0)) \\\n",
    "        .withColumn(f\"{tf}_mn_cos\", F.cos(2*math.pi * mn / 60.0))\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922e9665-173a-4c8b-8b78-2ae5481ed673",
   "metadata": {},
   "source": [
    "3. Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78f102f4-09c6-4157-92c5-4660a9d21ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [\"origin\", \"dest\", \"year\"]\n",
    "num_feats = [\"crselapsedtime\", \"distance\"]\n",
    "\n",
    "cyc_feats = (\n",
    "    [\"month_sin\", \"month_cos\", \"dayofmonth_sin\", \"dayofmonth_cos\", \"dayofweek_sin\", \"dayofweek_cos\"] +\n",
    "    [f\"{tf}_{p}_{axis}\"\n",
    "     for tf in time_features\n",
    "     for p in [\"hr\", \"mn\"]\n",
    "     for axis in [\"sin\", \"cos\"]]\n",
    ")\n",
    "\n",
    "all_numeric = num_feats + cyc_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f2642c3-d650-4086-b179-c1f8b672b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    for c in cats\n",
    "]\n",
    "ohe = OneHotEncoder(\n",
    "    inputCols=[f\"{c}_idx\" for c in cats],\n",
    "    outputCols=[f\"{c}_ohe\" for c in cats],\n",
    "    dropLast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4a969bd-d4bd-4fe1-b90c-c494a2896e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pre_vector_stages = indexers + [ohe]\n",
    "pre_vector_pipeline = Pipeline(stages=pre_vector_stages)\n",
    "pre_vector_model = pre_vector_pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8239379d-9f71-4275-8f81-07b717cda088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 16:59:21 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+-------------------+------------------+------------------+-------------------+-----------------------+-------------+-------------------+-----------------------+--------------------+-----------------------+-------------------+-----------------------+-------------------+-----------------------+----------------+----------------+-------------+---------+\n",
      "|crselapsedtime|distance|month_sin          |month_cos         |dayofmonth_sin    |dayofmonth_cos     |dayofweek_sin          |dayofweek_cos|crsdeptime_hr_sin  |crsdeptime_hr_cos      |crsdeptime_mn_sin   |crsdeptime_mn_cos      |crsarrtime_hr_sin  |crsarrtime_hr_cos      |crsarrtime_mn_sin  |crsarrtime_mn_cos      |origin_ohe      |dest_ohe        |year_ohe     |cancelled|\n",
      "+--------------+--------+-------------------+------------------+------------------+-------------------+-----------------------+-------------+-------------------+-----------------------+--------------------+-----------------------+-------------------+-----------------------+-------------------+-----------------------+----------------+----------------+-------------+---------+\n",
      "|130.0         |634.0   |-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.4492935982947064E-16|1.0          |-0.8660254037844386|0.5000000000000001     |-0.49999999999999967|0.8660254037844388     |-0.5000000000000004|0.8660254037844384     |0.49999999999999994|0.8660254037844387     |(362,[20],[1.0])|(360,[46],[1.0])|(3,[1],[1.0])|0.0      |\n",
      "|125.0         |634.0   |-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.4492935982947064E-16|1.0          |-0.4999999999999997|-0.8660254037844388    |5.66553889764798E-16|-1.0                   |-0.7071067811865471|-0.7071067811865479    |-0.4999999999999997|-0.8660254037844388    |(362,[20],[1.0])|(360,[46],[1.0])|(3,[1],[1.0])|0.0      |\n",
      "|175.0         |969.0   |-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.4492935982947064E-16|1.0          |-0.2588190451025208|-0.9659258262890683    |0.49999999999999994 |0.8660254037844387     |-0.7071067811865471|-0.7071067811865479    |0.0                |1.0                    |(362,[20],[1.0])|(360,[34],[1.0])|(3,[1],[1.0])|0.0      |\n",
      "|175.0         |969.0   |-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.4492935982947064E-16|1.0          |-1.0               |-1.8369701987210297E-16|-0.49999999999999967|0.8660254037844388     |-0.8660254037844386|0.5000000000000001     |-0.8660254037844386|0.5000000000000001     |(362,[20],[1.0])|(360,[34],[1.0])|(3,[1],[1.0])|0.0      |\n",
      "|180.0         |1012.0  |-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.4492935982947064E-16|1.0          |-0.8660254037844385|-0.5000000000000004    |-1.0                |-1.8369701987210297E-16|-1.0               |-1.8369701987210297E-16|-1.0               |-1.8369701987210297E-16|(362,[20],[1.0])|(360,[56],[1.0])|(3,[1],[1.0])|0.0      |\n",
      "|80.0          |356.0   |-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.4492935982947064E-16|1.0          |-0.9659258262890684|0.2588190451025203     |-0.49999999999999967|0.8660254037844388     |-0.7071067811865477|0.7071067811865474     |1.0                |2.83276944882399E-16   |(362,[20],[1.0])|(360,[69],[1.0])|(3,[1],[1.0])|0.0      |\n",
      "|80.0          |356.0   |-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.4492935982947064E-16|1.0          |0.258819045102521  |-0.9659258262890682    |-1.0                |-1.8369701987210297E-16|-0.2588190451025208|-0.9659258262890683    |0.49999999999999994|0.8660254037844387     |(362,[20],[1.0])|(360,[69],[1.0])|(3,[1],[1.0])|0.0      |\n",
      "|140.0         |719.0   |-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.4492935982947064E-16|1.0          |0.9659258262890683 |-0.25881904510252063   |-0.49999999999999967|0.8660254037844388     |0.7071067811865476 |-0.7071067811865475    |1.0                |2.83276944882399E-16   |(362,[20],[1.0])|(360,[30],[1.0])|(3,[1],[1.0])|0.0      |\n",
      "|145.0         |719.0   |-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.4492935982947064E-16|1.0          |0.258819045102521  |-0.9659258262890682    |-0.49999999999999967|0.8660254037844388     |-0.2588190451025208|-0.9659258262890683    |0.8660254037844387 |-0.4999999999999998    |(362,[20],[1.0])|(360,[30],[1.0])|(3,[1],[1.0])|0.0      |\n",
      "|140.0         |719.0   |-0.5000000000000004|0.8660254037844384|0.6513724827222223|-0.7587581226927909|-2.4492935982947064E-16|1.0          |-1.0               |-1.8369701987210297E-16|-0.49999999999999967|0.8660254037844388     |-0.8660254037844386|0.5000000000000001     |1.0                |2.83276944882399E-16   |(362,[20],[1.0])|(360,[30],[1.0])|(3,[1],[1.0])|0.0      |\n",
      "+--------------+--------+-------------------+------------------+------------------+-------------------+-----------------------+-------------+-------------------+-----------------------+--------------------+-----------------------+-------------------+-----------------------+-------------------+-----------------------+----------------+----------------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw_plus_ohe = pre_vector_model.transform(df)\n",
    "intermediate_cols = all_numeric + [f\"{c}_ohe\" for c in cats] + [\"cancelled\"]\n",
    "df_ohe = df_raw_plus_ohe.select(intermediate_cols)\n",
    "\n",
    "df_ohe.createOrReplaceTempView(\"flight_features_intermediate\")\n",
    "spark.sql(\"SELECT * FROM flight_features_intermediate LIMIT 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb4f0f-a9e1-4e2c-9758-6d6f4704cabc",
   "metadata": {},
   "source": [
    "4. Assemble numeric + cyclical features, then scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b1b0a9-0671-4402-a1e0-6395c5ffc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=all_numeric + [f\"{c}_ohe\" for c in cats], outputCol=\"features_raw\")\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withMean=True, withStd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "239489c6-b6d8-4ef7-9ac1-5dff3f36d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=indexers + [ohe, assembler, scaler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bf1f20b-116a-4ab6-87d8-51656cb6b530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(df)\n",
    "df_prepared = model.transform(df).select(\"features\", \"cancelled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bda79a04-4ca3-4bcc-9326-077451526c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|cancelled|\n",
      "+--------------------+---------+\n",
      "|[-0.1884130372273...|      0.0|\n",
      "|[-0.2546376588361...|      0.0|\n",
      "|[0.40760855725178...|      0.0|\n",
      "|[0.40760855725178...|      0.0|\n",
      "|[0.47383317886057...|      0.0|\n",
      "|[-0.8506592533152...|      0.0|\n",
      "|[-0.8506592533152...|      0.0|\n",
      "|[-0.0559637940097...|      0.0|\n",
      "|[0.01026082759904...|      0.0|\n",
      "|[-0.0559637940097...|      0.0|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_prepared.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc977a0-741d-47e3-9282-b166be036d84",
   "metadata": {},
   "source": [
    "5. Balance dataset by down-sampling the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53ebf370-087e-4119-9981-a9c1812f0075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 18240580, 1.0: 265122}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "counts = df_prepared.groupBy(\"cancelled\").count().collect()\n",
    "cnts = {row[\"cancelled\"]: row[\"count\"] for row in counts}\n",
    "print(cnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "493ce452-9309-4f45-b783-4090f7ed0bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0: 1.0, 0.0: 0.014534735189341567}\n"
     ]
    }
   ],
   "source": [
    "min_class = min(cnts, key=cnts.get)\n",
    "maj_class = max(cnts, key=cnts.get)\n",
    "fraction = float(cnts[min_class]) / float(cnts[maj_class])\n",
    "fractions = {min_class: 1.0, maj_class: fraction}\n",
    "print(fractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2270fde2-2d64-48c9-aeef-378935ebc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = df_prepared.sampleBy(\"cancelled\", fractions, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fac7f997-152c-4535-9333-caee5776b318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|cancelled|\n",
      "+--------------------+---------+\n",
      "|[0.01026082759904...|      0.0|\n",
      "|[0.01026082759904...|      0.0|\n",
      "|[-0.2546376588361...|      0.0|\n",
      "|[-0.8506592533152...|      0.0|\n",
      "|[-0.0559637940097...|      0.0|\n",
      "|[0.27515931403420...|      0.0|\n",
      "|[-1.0493331181415...|      0.0|\n",
      "|[-1.1155577397503...|      1.0|\n",
      "|[1.59965174621000...|      0.0|\n",
      "|[0.01026082759904...|      0.0|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_balanced.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9eb7b277-b593-4e17-8295-3b7f56efa6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after balancing dataset: 531167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows after balancing dataset: {df_balanced.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be72901f-fe34-4d85-8f4c-42f306c944e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 266045, 1.0: 265122}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "counts = df_balanced.groupBy(\"cancelled\").count().collect()\n",
    "cnts = {row[\"cancelled\"]: row[\"count\"] for row in counts}\n",
    "print(cnts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f4b91b-7eff-401a-9c44-c4af2afb132d",
   "metadata": {},
   "source": [
    "6. Split dataset into train / val (with stratify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24d5f486-60a3-4587-9ae1-7d3355b51e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:====================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 425472 Valid size: 105695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_class_0 = df_balanced.filter(F.col(\"cancelled\") == 0.0)\n",
    "df_class_1 = df_balanced.filter(F.col(\"cancelled\") == 1.0)\n",
    "train_0, val_0 = df_class_0.randomSplit([0.8, 0.2], seed=42)\n",
    "train_1, val_1 = df_class_1.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_df = train_0.unionAll(train_1)\n",
    "val_df = val_0.unionAll(val_1)\n",
    "print(\"Train size:\", train_df.count(), \"Valid size:\", val_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7765a84a-1de3-41be-91fa-dd8d046f7501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:====================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 213099, 1.0: 212373}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "counts = train_df.groupBy(\"cancelled\").count().collect()\n",
    "cnts = {row[\"cancelled\"]: row[\"count\"] for row in counts}\n",
    "print(cnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d5bf714-377f-463e-ab01-8f2233222789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|cancelled|\n",
      "+--------------------+---------+\n",
      "|[-1.6321097882989...|      0.0|\n",
      "|[-1.6188648639771...|      0.0|\n",
      "|[-1.5393953180466...|      0.0|\n",
      "|[-1.5393953180466...|      0.0|\n",
      "|[-1.4996605450813...|      0.0|\n",
      "|[-1.4864156207596...|      0.0|\n",
      "|[-1.4864156207596...|      0.0|\n",
      "|[-1.4864156207596...|      0.0|\n",
      "|[-1.4731706964378...|      0.0|\n",
      "|[-1.4731706964378...|      0.0|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce020bc1-affc-4589-9bd7-0e950c21a046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|cancelled|\n",
      "+--------------------+---------+\n",
      "|[-1.6056199396554...|      0.0|\n",
      "|[-1.4864156207596...|      0.0|\n",
      "|[-1.4864156207596...|      0.0|\n",
      "|[-1.4731706964378...|      0.0|\n",
      "|[-1.4599257721160...|      0.0|\n",
      "|[-1.4599257721160...|      0.0|\n",
      "|[-1.4599257721160...|      0.0|\n",
      "|[-1.4466808477943...|      0.0|\n",
      "|[-1.4466808477943...|      0.0|\n",
      "|[-1.4466808477943...|      0.0|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "val_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be07aa-6e9e-429c-bea7-5dfdbbad9f08",
   "metadata": {},
   "source": [
    "7. Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c3ea8ea-551b-49a5-97eb-25443562b349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/team15/BigData-project\n",
      "\u001b[0m\u001b[01;34mdata\u001b[0m/    \u001b[01;34mmodels\u001b[0m/     \u001b[01;34moutput\u001b[0m/    recovered-file-0.csv  \u001b[01;34mscripts\u001b[0m/  \u001b[01;34msql\u001b[0m/\n",
      "main.sh  \u001b[01;34mnotebooks\u001b[0m/  README.MD  requirements.txt      \u001b[01;34msecrets\u001b[0m/  \u001b[01;34mvenv\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cfc3351-20ce-4532-a0a8-542e3a69b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "56ba10dd-de8c-40f9-bf4f-2dfe215bc799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(command):\n",
    "    return os.popen(command).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0dadb385-6649-4706-bbf7-5a349334c0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.select(\"features\", \"cancelled\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/train\")\n",
    "\n",
    "run(\"hdfs dfs -cat project/data/train/*.json > data/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "867ad247-8d71-41cf-8874-b700f2600d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.select(\"features\", \"cancelled\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/val\")\n",
    "\n",
    "run(\"hdfs dfs -cat project/data/val/*.json > data/val.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab9b1b",
   "metadata": {},
   "source": [
    "### ML modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5e4fcc-cf0b-4f4e-a7a1-f19b40d99339",
   "metadata": {},
   "source": [
    "#### Model 1: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94f79b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = \"cancelled\"\n",
    "features_col = \"features\"\n",
    "\n",
    "lr = LogisticRegression(labelCol=label_col, featuresCol=features_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20c726d3-9002-42d9-9c99-bd260e083e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.maxIter, [10, 50]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f78757a0-7704-4e84-b27d-3ce1337d416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator1 = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2498eb5-a4f5-482c-b389-d0a52bd3f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_lr = CrossValidator(estimator=lr,\n",
    "                       estimatorParamMaps=paramGrid_lr,\n",
    "                       evaluator=evaluator1,\n",
    "                       numFolds=3,\n",
    "                       parallelism=5,\n",
    "                       seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9996018f-88b4-4663-a1d7-18e244937f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression with Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 17:43:50 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/05/08 18:31:53 ERROR TransportClient: Failed to send RPC RPC 6296566509472130664 to /10.100.30.60:57858: io.netty.channel.StacklessClosedChannelException\n",
      "io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5139_1 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5424_7 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5424_2 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5175_6 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5139_12 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5424_5 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5185_10 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5175_1 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5139_4 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5163_4 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5065_10 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5175_4 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5139_10 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5537_8 !\n",
      "25/05/08 18:31:53 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to get executor loss reason for executor id 1 at RPC address 10.100.30.60:57906, but got no response. Marking as agent lost.\n",
      "java.io.IOException: Failed to send RPC RPC 6296566509472130664 to /10.100.30.60:57858: io.netty.channel.StacklessClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:395)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:372)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:877)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:940)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1247)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5163_10 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5185_4 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5065_3 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5424_9 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5175_10 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5163_6 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5185_3 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5537_1 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5537_10 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5424_11 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5175_8 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5139_8 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5537_4 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5139_3 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5163_8 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5175_12 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5065_6 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5424_13 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5163_3 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5065_8 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5139_6 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5185_6 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5065_4 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5424_0 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5537_3 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5185_1 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5065_1 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5163_1 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5537_6 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5175_3 !\n",
      "25/05/08 18:31:53 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5185_8 !\n",
      "25/05/08 18:31:53 ERROR YarnScheduler: Lost executor 1 on hadoop-04.uni.innopolis.ru: Executor Process Lost\n",
      "25/05/08 18:31:53 WARN TaskSetManager: Lost task 10.0 in stage 2119.0 (TID 17228) (hadoop-04.uni.innopolis.ru executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor Process Lost\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5163_0 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5175_7 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5065_2 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5163_13 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5185_12 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5185_0 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5185_5 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5537_11 !\n",
      "25/05/08 18:31:54 ERROR TransportClient: Failed to send RPC RPC 6607148507069004764 to /10.100.30.60:57858: io.netty.channel.StacklessClosedChannelException\n",
      "io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5537_9 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5537_13 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5139_7 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5139_9 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5424_1 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5185_11 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5065_12 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5175_11 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5185_7 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5065_7 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5139_13 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5065_11 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5163_7 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5163_12 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5139_5 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5139_0 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5185_9 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5163_9 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5424_8 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5065_13 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5139_11 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5424_3 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5537_0 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5175_2 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5424_10 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5065_9 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5537_5 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5163_2 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5424_4 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5424_12 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5537_7 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5139_2 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5163_5 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5424_6 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5163_11 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5185_2 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5185_13 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5175_5 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5537_2 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5175_9 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5065_0 !\n",
      "25/05/08 18:31:54 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to get executor loss reason for executor id 2 at RPC address 10.100.30.59:59060, but got no response. Marking as agent lost.\n",
      "java.io.IOException: Failed to send RPC RPC 6607148507069004764 to /10.100.30.60:57858: io.netty.channel.StacklessClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:395)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:372)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:877)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:940)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1247)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5175_13 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5537_12 !\n",
      "25/05/08 18:31:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_5065_5 !\n",
      "25/05/08 18:31:54 ERROR YarnScheduler: Lost executor 2 on hadoop-03.uni.innopolis.ru: Executor Process Lost\n",
      "25/05/08 18:31:54 WARN TaskSetManager: Lost task 0.0 in stage 2119.0 (TID 17231) (hadoop-03.uni.innopolis.ru executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor Process Lost\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression training complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Logistic Regression with Cross-Validation...\")\n",
    "cv_model_lr = cv_lr.fit(train_df)\n",
    "print(\"Logistic Regression training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d274f5b9-7c8e-4943-95b3-ee337beaab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr_model = cv_model_lr.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b886cad-7287-4c48-adc4-ac751c886224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Logistic Regression Model Parameters:\n",
      "  elasticNetParam: 0.0\n",
      "  maxIter: 50\n",
      "  regParam: 0.01\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBest Logistic Regression Model Parameters:\")\n",
    "for param in best_lr_model.extractParamMap():\n",
    "    if param.name in ['regParam', 'elasticNetParam', 'maxIter']:\n",
    "        print(f\"  {param.name}: {best_lr_model.getOrDefault(param)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e7c0550-aa1d-43f6-84ab-cd69df6d9b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr = best_lr_model.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6be3fd40-fb12-46ae-bee4-5d8ff2226af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Test AUC: 0.6958213003410854\n"
     ]
    }
   ],
   "source": [
    "auc_lr_test = evaluator1.evaluate(predictions_lr)\n",
    "print(f\"Logistic Regression - Test AUC: {auc_lr_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "941de998-5956-4ae7-871e-31a3f6deda7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Metrics for Logistic Regression on Test Data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "[Stage 2884:==================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Confusion Matrix:\n",
      "[[33428. 19518.]\n",
      " [18366. 34383.]]\n",
      "  Precision (Cancelled=1.0): 0.6379\n",
      "  Recall (Cancelled=1.0): 0.6518\n",
      "  F1-Score (Cancelled=1.0): 0.6448\n",
      "  Accuracy: 0.6416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def get_detailed_metrics(predictions_df, model_name=\"Model\"):\n",
    "    print(f\"\\nDetailed Metrics for {model_name} on Test Data:\")\n",
    "\n",
    "    preds_and_labels = predictions_df.select(\n",
    "        F.col(\"prediction\").cast(DoubleType()),\n",
    "        F.col(label_col).cast(DoubleType())\n",
    "    ).rdd.map(lambda r: (r[0], r[1]))\n",
    "\n",
    "    metrics = MulticlassMetrics(preds_and_labels)\n",
    "    print(f\"  Confusion Matrix:\\n{metrics.confusionMatrix().toArray()}\")\n",
    "    print(f\"  Precision (Cancelled=1.0): {metrics.precision(1.0):.4f}\")\n",
    "    print(f\"  Recall (Cancelled=1.0): {metrics.recall(1.0):.4f}\")\n",
    "    print(f\"  F1-Score (Cancelled=1.0): {metrics.fMeasure(1.0):.4f}\")\n",
    "    print(f\"  Accuracy: {metrics.accuracy:.4f}\")\n",
    "    return metrics.accuracy, metrics.precision(1.0), metrics.recall(1.0), metrics.fMeasure(1.0)\n",
    "\n",
    "\n",
    "lr_accuracy, lr_precision, lr_recall, lr_f1 = get_detailed_metrics(predictions_lr, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c0f691-f2e3-44de-b583-96cef98a5eab",
   "metadata": {},
   "source": [
    " Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b508c146-5652-4db1-bcd4-1cf33b1d9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='LogisticRegression_9942d619e73a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
      " Param(parent='LogisticRegression_9942d619e73a', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
      " Param(parent='LogisticRegression_9942d619e73a', name='labelCol', doc='label column name.'): 'cancelled',\n",
      " Param(parent='LogisticRegression_9942d619e73a', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
      " Param(parent='LogisticRegression_9942d619e73a', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
      " Param(parent='LogisticRegression_9942d619e73a', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
      " Param(parent='LogisticRegression_9942d619e73a', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
      " Param(parent='LogisticRegression_9942d619e73a', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
      " Param(parent='LogisticRegression_9942d619e73a', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto',\n",
      " Param(parent='LogisticRegression_9942d619e73a', name='maxIter', doc='max number of iterations (>= 0).'): 50,\n",
      " Param(parent='LogisticRegression_9942d619e73a', name='featuresCol', doc='features column name.'): 'features',\n",
      " Param(parent='LogisticRegression_9942d619e73a', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
      " Param(parent='LogisticRegression_9942d619e73a', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
      " Param(parent='LogisticRegression_9942d619e73a', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06,\n",
      " Param(parent='LogisticRegression_9942d619e73a', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "model_output_path_lr = f\"project/models/logistic_regression_model\"\n",
    "best_lr_model = cv_model_lr.bestModel\n",
    "pprint(best_lr_model.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e27df012-ac1c-4204-bda8-65d9bea242c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best Logistic Regression model to: project/models/logistic_regression_model\n"
     ]
    }
   ],
   "source": [
    "best_lr_model.write().overwrite().save(model_output_path_lr)\n",
    "run(\"hdfs dfs -get project/models/logistic_regression_model models/logistic_regression_model\")\n",
    "print(f\"Saved best Logistic Regression model to: {model_output_path_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acdcc65-fbac-4eda-9d63-3efb05bf49d6",
   "metadata": {},
   "source": [
    "Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b2040a87-47ba-4935-bcfb-e87574176963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Logistic Regression predictions to: project/output/lr_model_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "predictions_output_path_lr = f\"project/output/lr_model_predictions.csv\"\n",
    "\n",
    "predictions_lr.select(label_col, \"prediction\") \\\n",
    "    .coalesce(1) \\\n",
    "    .write.mode(\"overwrite\").format(\"csv\") \\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(predictions_output_path_lr)\n",
    "\n",
    "run(\"hdfs dfs -cat project/output/lr_model_predictions.csv/*.csv > output/lr_model_predictions.csv\")\n",
    "print(f\"Saved Logistic Regression predictions to: {predictions_output_path_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3258722b-9b8d-4378-919f-0a489d45c2e0",
   "metadata": {},
   "source": [
    "#### Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "90fc3de6-b94e-44cc-9258-e7f8e5b4df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=label_col, featuresCol=features_col, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ac24f484-e848-4438-9191-367c5c71e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [20, 50]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(rf.featureSubsetStrategy, [\"sqrt\", \"log2\"]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f53bc0f9-712d-42f1-8dc4-846e87317bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator2 = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e3155335-5b15-4ce2-bd9c-eec82ca4c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_rf = CrossValidator(estimator=rf,\n",
    "                       estimatorParamMaps=paramGrid_rf,\n",
    "                       evaluator=evaluator2,\n",
    "                       numFolds=3,\n",
    "                       parallelism=5,\n",
    "                       seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "948c733a-8954-4201-8a5c-5753706dc92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest with Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 20:18:51 WARN CacheManager: Asked to cache already cached data.\n",
      "25/05/08 20:18:51 WARN CacheManager: Asked to cache already cached data.\n",
      "25/05/08 20:31:21 WARN DAGScheduler: Broadcasting large task binary with size 1247.4 KiB\n",
      "25/05/08 20:31:24 WARN DAGScheduler: Broadcasting large task binary with size 1247.4 KiB\n",
      "25/05/08 20:31:27 WARN DAGScheduler: Broadcasting large task binary with size 1247.4 KiB\n",
      "25/05/08 20:31:31 WARN DAGScheduler: Broadcasting large task binary with size 1247.4 KiB\n",
      "25/05/08 20:31:42 WARN DAGScheduler: Broadcasting large task binary with size 1579.1 KiB\n",
      "25/05/08 20:31:45 WARN DAGScheduler: Broadcasting large task binary with size 1579.2 KiB\n",
      "25/05/08 20:31:57 WARN DAGScheduler: Broadcasting large task binary with size 2001.2 KiB\n",
      "25/05/08 20:32:01 WARN DAGScheduler: Broadcasting large task binary with size 2001.2 KiB\n",
      "25/05/08 20:32:10 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/05/08 20:32:13 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/05/08 20:32:31 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/08 20:32:35 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/05/08 20:33:01 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/05/08 20:33:05 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/05/08 20:33:50 WARN DAGScheduler: Broadcasting large task binary with size 1979.4 KiB\n",
      "25/05/08 20:33:50 WARN DAGScheduler: Broadcasting large task binary with size 1979.4 KiB\n",
      "25/05/08 20:37:15 WARN DAGScheduler: Broadcasting large task binary with size 1352.6 KiB\n",
      "25/05/08 20:37:21 WARN DAGScheduler: Broadcasting large task binary with size 1352.6 KiB\n",
      "25/05/08 20:37:33 WARN DAGScheduler: Broadcasting large task binary with size 1822.1 KiB\n",
      "25/05/08 20:37:38 WARN DAGScheduler: Broadcasting large task binary with size 1822.2 KiB\n",
      "25/05/08 20:37:57 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/05/08 20:38:01 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/05/08 20:38:49 WARN DAGScheduler: Broadcasting large task binary with size 1362.6 KiB\n",
      "25/05/08 20:38:49 WARN DAGScheduler: Broadcasting large task binary with size 1362.6 KiB\n",
      "25/05/08 20:40:37 WARN DAGScheduler: Broadcasting large task binary with size 1352.6 KiB\n",
      "25/05/08 20:40:41 WARN DAGScheduler: Broadcasting large task binary with size 1352.6 KiB\n",
      "25/05/08 20:40:53 WARN DAGScheduler: Broadcasting large task binary with size 1822.2 KiB\n",
      "25/05/08 20:41:01 WARN DAGScheduler: Broadcasting large task binary with size 1822.1 KiB\n",
      "25/05/08 20:41:19 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/05/08 20:41:28 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/05/08 20:41:46 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/05/08 20:41:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/05/08 20:42:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/05/08 20:42:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/05/08 20:42:47 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/05/08 20:42:57 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/05/08 20:43:16 WARN DAGScheduler: Broadcasting large task binary with size 6.8 MiB\n",
      "25/05/08 20:43:27 WARN DAGScheduler: Broadcasting large task binary with size 6.8 MiB\n",
      "25/05/08 20:43:50 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "25/05/08 20:44:02 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "25/05/08 20:44:12 WARN DAGScheduler: Broadcasting large task binary with size 1007.2 KiB\n",
      "25/05/08 20:44:45 WARN DAGScheduler: Broadcasting large task binary with size 1007.2 KiB\n",
      "25/05/08 20:44:49 WARN DAGScheduler: Broadcasting large task binary with size 1138.4 KiB\n",
      "25/05/08 20:45:00 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/05/08 20:45:00 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/05/08 20:45:05 WARN DAGScheduler: Broadcasting large task binary with size 1138.5 KiB\n",
      "25/05/08 20:45:12 WARN DAGScheduler: Broadcasting large task binary with size 1285.3 KiB\n",
      "25/05/08 20:45:28 WARN DAGScheduler: Broadcasting large task binary with size 1285.3 KiB\n",
      "25/05/08 20:45:32 WARN DAGScheduler: Broadcasting large task binary with size 1436.2 KiB\n",
      "25/05/08 20:45:45 WARN DAGScheduler: Broadcasting large task binary with size 1436.2 KiB\n",
      "25/05/08 20:45:50 WARN DAGScheduler: Broadcasting large task binary with size 1606.0 KiB\n",
      "25/05/08 20:45:59 WARN DAGScheduler: Broadcasting large task binary with size 1606.0 KiB\n",
      "25/05/08 20:46:14 WARN DAGScheduler: Broadcasting large task binary with size 1147.2 KiB\n",
      "25/05/08 20:46:28 WARN DAGScheduler: Broadcasting large task binary with size 1626.6 KiB\n",
      "25/05/08 20:46:47 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/05/08 20:47:12 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/05/08 20:47:48 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/05/08 20:48:15 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/05/08 20:56:07 WARN DAGScheduler: Broadcasting large task binary with size 1147.2 KiB\n",
      "25/05/08 20:56:25 WARN DAGScheduler: Broadcasting large task binary with size 1626.6 KiB\n",
      "25/05/08 20:56:52 WARN DAGScheduler: Broadcasting large task binary with size 1023.4 KiB\n",
      "25/05/08 20:57:01 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/05/08 20:57:30 WARN DAGScheduler: Broadcasting large task binary with size 1023.4 KiB\n",
      "25/05/08 20:57:36 WARN DAGScheduler: Broadcasting large task binary with size 1198.2 KiB\n",
      "25/05/08 20:57:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/05/08 20:58:28 WARN DAGScheduler: Broadcasting large task binary with size 1198.2 KiB\n",
      "25/05/08 20:58:34 WARN DAGScheduler: Broadcasting large task binary with size 1395.9 KiB\n",
      "25/05/08 20:58:48 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/05/08 20:59:06 WARN DAGScheduler: Broadcasting large task binary with size 1395.9 KiB\n",
      "25/05/08 20:59:28 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/05/08 21:00:17 WARN DAGScheduler: Broadcasting large task binary with size 1621.3 KiB\n",
      "25/05/08 21:03:02 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "25/05/08 21:03:16 WARN DAGScheduler: Broadcasting large task binary with size 1872.2 KiB\n",
      "25/05/08 21:03:27 WARN DAGScheduler: Broadcasting large task binary with size 1021.6 KiB\n",
      "25/05/08 21:03:30 WARN DAGScheduler: Broadcasting large task binary with size 10.5 MiB\n",
      "25/05/08 21:03:46 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/05/08 21:03:58 WARN DAGScheduler: Broadcasting large task binary with size 1265.8 KiB\n",
      "25/05/08 21:04:32 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/05/08 21:07:20 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/05/08 21:07:35 WARN DAGScheduler: Broadcasting large task binary with size 1529.4 KiB\n",
      "25/05/08 21:07:38 WARN DAGScheduler: Broadcasting large task binary with size 17.2 MiB\n",
      "25/05/08 21:07:53 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/05/08 21:08:08 WARN DAGScheduler: Broadcasting large task binary with size 1822.2 KiB\n",
      "25/05/08 21:08:12 WARN DAGScheduler: Broadcasting large task binary with size 8.3 MiB\n",
      "25/05/08 21:08:51 WARN DAGScheduler: Broadcasting large task binary with size 1569.6 KiB\n",
      "25/05/08 21:11:29 WARN DAGScheduler: Broadcasting large task binary with size 1235.5 KiB\n",
      "25/05/08 21:11:33 WARN DAGScheduler: Broadcasting large task binary with size 1235.5 KiB\n",
      "25/05/08 21:14:39 WARN DAGScheduler: Broadcasting large task binary with size 1578.5 KiB\n",
      "25/05/08 21:14:48 WARN DAGScheduler: Broadcasting large task binary with size 1993.9 KiB\n",
      "25/05/08 21:14:52 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/05/08 21:15:05 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/08 21:15:19 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/05/08 21:15:43 WARN DAGScheduler: Broadcasting large task binary with size 1989.2 KiB\n",
      "25/05/08 21:17:31 WARN DAGScheduler: Broadcasting large task binary with size 1011.4 KiB\n",
      "25/05/08 21:17:42 WARN DAGScheduler: Broadcasting large task binary with size 1362.9 KiB\n",
      "25/05/08 21:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1821.0 KiB\n",
      "25/05/08 21:18:11 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/05/08 21:18:36 WARN DAGScheduler: Broadcasting large task binary with size 1336.4 KiB\n",
      "25/05/08 21:19:18 WARN DAGScheduler: Broadcasting large task binary with size 1011.4 KiB\n",
      "25/05/08 21:19:31 WARN DAGScheduler: Broadcasting large task binary with size 1362.9 KiB\n",
      "25/05/08 21:19:38 WARN DAGScheduler: Broadcasting large task binary with size 1821.0 KiB\n",
      "25/05/08 21:19:48 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/05/08 21:19:56 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/08 21:20:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/05/08 21:20:14 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/05/08 21:20:26 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/05/08 21:20:37 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "25/05/08 21:20:55 WARN DAGScheduler: Broadcasting large task binary with size 1036.0 KiB\n",
      "25/05/08 21:20:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/05/08 21:21:00 WARN DAGScheduler: Broadcasting large task binary with size 1177.2 KiB\n",
      "25/05/08 21:21:07 WARN DAGScheduler: Broadcasting large task binary with size 1339.5 KiB\n",
      "25/05/08 21:21:11 WARN DAGScheduler: Broadcasting large task binary with size 1513.4 KiB\n",
      "25/05/08 21:21:16 WARN DAGScheduler: Broadcasting large task binary with size 1711.2 KiB\n",
      "25/05/08 21:21:21 WARN DAGScheduler: Broadcasting large task binary with size 1029.9 KiB\n",
      "25/05/08 21:30:55 WARN DAGScheduler: Broadcasting large task binary with size 1242.1 KiB\n",
      "25/05/08 21:31:01 WARN DAGScheduler: Broadcasting large task binary with size 1242.1 KiB\n",
      "25/05/08 21:31:09 WARN DAGScheduler: Broadcasting large task binary with size 1591.5 KiB\n",
      "25/05/08 21:31:17 WARN DAGScheduler: Broadcasting large task binary with size 2019.2 KiB\n",
      "25/05/08 21:31:21 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/05/08 21:31:28 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/05/08 21:31:38 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/05/08 21:31:55 WARN DAGScheduler: Broadcasting large task binary with size 2042.4 KiB\n",
      "25/05/08 21:34:11 WARN DAGScheduler: Broadcasting large task binary with size 1010.6 KiB\n",
      "25/05/08 21:34:25 WARN DAGScheduler: Broadcasting large task binary with size 1381.9 KiB\n",
      "25/05/08 21:34:36 WARN DAGScheduler: Broadcasting large task binary with size 1885.8 KiB\n",
      "25/05/08 21:34:47 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/05/08 21:35:09 WARN DAGScheduler: Broadcasting large task binary with size 1414.6 KiB\n",
      "25/05/08 21:36:15 WARN DAGScheduler: Broadcasting large task binary with size 1010.6 KiB\n",
      "25/05/08 21:36:22 WARN DAGScheduler: Broadcasting large task binary with size 1381.9 KiB\n",
      "25/05/08 21:36:29 WARN DAGScheduler: Broadcasting large task binary with size 1885.8 KiB\n",
      "25/05/08 21:36:36 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/05/08 21:36:44 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "25/05/08 21:36:53 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/05/08 21:37:03 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "25/05/08 21:37:13 WARN DAGScheduler: Broadcasting large task binary with size 7.2 MiB\n",
      "25/05/08 21:37:24 WARN DAGScheduler: Broadcasting large task binary with size 9.0 MiB\n",
      "25/05/08 21:37:27 WARN DAGScheduler: Broadcasting large task binary with size 1094.9 KiB\n",
      "25/05/08 21:37:38 WARN DAGScheduler: Broadcasting large task binary with size 1232.8 KiB\n",
      "25/05/08 21:37:43 WARN DAGScheduler: Broadcasting large task binary with size 1386.1 KiB\n",
      "25/05/08 21:37:47 WARN DAGScheduler: Broadcasting large task binary with size 1545.3 KiB\n",
      "25/05/08 21:37:48 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/05/08 21:50:46 WARN DAGScheduler: Broadcasting large task binary with size 1019.4 KiB\n",
      "25/05/08 21:50:50 WARN DAGScheduler: Broadcasting large task binary with size 1391.1 KiB\n",
      "25/05/08 21:50:56 WARN DAGScheduler: Broadcasting large task binary with size 1894.4 KiB\n",
      "25/05/08 21:51:02 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/05/08 21:51:09 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "25/05/08 21:51:17 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "25/05/08 21:51:27 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/05/08 21:51:37 WARN DAGScheduler: Broadcasting large task binary with size 7.5 MiB\n",
      "25/05/08 21:51:49 WARN DAGScheduler: Broadcasting large task binary with size 9.5 MiB\n",
      "25/05/08 21:52:00 WARN DAGScheduler: Broadcasting large task binary with size 1018.7 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest training complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Random Forest with Cross-Validation...\")\n",
    "cv_model_rf = cv_rf.fit(train_df)\n",
    "print(\"Random Forest training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9ab0c448-cb08-47b9-a5a9-9493fc9fbfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = cv_model_rf.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9aa3ff99-1486-4e32-9234-da16a8e1c47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Random Forest Model Parameters:\n",
      "  featureSubsetStrategy: sqrt\n",
      "  maxDepth: 15\n",
      "  numTrees: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBest Random Forest Model Parameters:\")\n",
    "for param in best_rf_model.extractParamMap():\n",
    "     if param.name in ['numTrees', 'maxDepth', 'featureSubsetStrategy']:\n",
    "        print(f\"  {param.name}: {best_rf_model.getOrDefault(param)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "da4322bf-556b-4f8d-8ecd-693ea821c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rf = best_rf_model.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ba08c9e8-f34d-4f8a-9bd9-3fd4a36060cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 21:52:03 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Test AUC: 0.7649094722448742\n"
     ]
    }
   ],
   "source": [
    "auc_rf_test = evaluator2.evaluate(predictions_rf)\n",
    "print(f\"Random Forest - Test AUC: {auc_rf_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dba81fb7-d816-4e8c-b289-3eea283aa8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Metrics for Random Forest on Test Data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "25/05/08 21:55:07 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/05/08 21:55:40 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n",
      "[Stage 5117:==================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Confusion Matrix:\n",
      "[[36710. 16236.]\n",
      " [15939. 36810.]]\n",
      "  Precision (Cancelled=1.0): 0.6939\n",
      "  Recall (Cancelled=1.0): 0.6978\n",
      "  F1-Score (Cancelled=1.0): 0.6959\n",
      "  Accuracy: 0.6956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rf_accuracy, rf_precision, rf_recall, rf_f1 = get_detailed_metrics(predictions_rf, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9730dd-d662-4d09-a41d-cadbf21bce22",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e665544d-b767-4263-8043-4d53a4f7c36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='RandomForestClassifier_7dbb85a09305', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False,\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='bootstrap', doc='Whether bootstrap samples are used when building trees.'): True,\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='numTrees', doc='Number of trees to train (>= 1).'): 50,\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='seed', doc='random seed.'): 42,\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='featureSubsetStrategy', doc=\"The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto'\"): 'sqrt',\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10,\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='featuresCol', doc='features column name.'): 'features',\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0,\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='labelCol', doc='label column name.'): 'cancelled',\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256,\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0,\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 15,\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '',\n",
      " Param(parent='RandomForestClassifier_7dbb85a09305', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32}\n"
     ]
    }
   ],
   "source": [
    "model_output_path_rf = f\"project/models/random_forest_model\"\n",
    "best_rf_model = cv_model_rf.bestModel\n",
    "pprint(best_rf_model.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f2ba85f6-0a62-406a-abca-6c36a9b71992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 21:59:06 WARN TaskSetManager: Stage 5123 contains a task of very large size (2231 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best Random Forest model to: project/models/random_forest_model\n"
     ]
    }
   ],
   "source": [
    "best_rf_model.write().overwrite().save(model_output_path_rf)\n",
    "run(\"hdfs dfs -get project/models/random_forest_model models/random_forest_model\")\n",
    "print(f\"Saved best Random Forest model to: {model_output_path_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7955c94-becb-4783-9338-337b04be8472",
   "metadata": {},
   "source": [
    "Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a3e73b6b-f05c-49a3-81fb-ea31f9738bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 21:59:09 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Random Forest predictions to: project/output/rf_model_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "predictions_output_path_rf = f\"project/output/rf_model_predictions.csv\"\n",
    "\n",
    "predictions_rf.select(label_col, \"prediction\") \\\n",
    "    .coalesce(1) \\\n",
    "    .write.mode(\"overwrite\").format(\"csv\") \\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(predictions_output_path_rf)\n",
    "\n",
    "run(\"hdfs dfs -cat project/output/rf_model_predictions.csv/*.csv > output/rf_model_predictions.csv\")\n",
    "print(f\"Saved Random Forest predictions to: {predictions_output_path_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e266fa33-b24e-4ffa-9cc7-b318269885dd",
   "metadata": {},
   "source": [
    "#### Comapare best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c67be-a7e3-47d9-adb7-0e542bea2e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|Model                                                                                                               |AUC               |Accuracy          |Precision         |Recall            |F1-score          |\n",
      "+--------------------------------------------------------------------------------------------------------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|LogisticRegressionModel: uid=LogisticRegression_9942d619e73a, numClasses=2, numFeatures=741                         |0.6958213003410854|0.6415724490278631|0.6378916903211443|0.6518227833703009|0.6447819971870605|\n",
      "|RandomForestClassificationModel: uid=RandomForestClassifier_7dbb85a09305, numTrees=50, numClasses=2, numFeatures=741|0.7649094722448742|0.6955863569705284|0.6939260264675942|0.6978331342774271|0.6958740961293067|\n",
      "+--------------------------------------------------------------------------------------------------------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    [str(best_lr_model), auc_lr_test, lr_accuracy, lr_precision, lr_recall, lr_f1],\n",
    "    [str(best_rf_model), auc_rf_test, rf_accuracy, rf_precision, rf_recall, rf_f1]\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(models, [\"Model\", \"AUC\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ae0cec8-8494-4ad4-8823-35c4d762fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "871d894a-c739-4320-8714-d108ce21cfdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"hdfs dfs -cat project/output/evaluation.csv/*.csv > output/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c16e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disconnect from Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ce06d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
