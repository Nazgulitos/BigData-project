{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb0ed7a",
   "metadata": {},
   "source": [
    "## Stage 3: Predictive Data Analytics\n",
    "\n",
    "### Imports and Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc12d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "team = \"team15\"\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(f\"{team} - spark ML\")\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3f496",
   "metadata": {},
   "source": [
    "### Load data from Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42bdc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all databases\n",
    "\n",
    "spark.sql(\"SHOW DATABASES;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ab429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables\n",
    "\n",
    "hive_db_name = \"team15_projectdb\"\n",
    "spark.sql(f\"USE {hive_db_name};\")\n",
    "spark.sql(\"SHOW TABLES;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Hive tables\n",
    "\n",
    "airport = spark.read.format(\"avro\").table(f'{hive_db_name}.airport')\n",
    "flight = spark.read.format(\"avro\").table(f'{hive_db_name}.flight')\n",
    "cancellationreason = spark.read.format(\"avro\").table(f'{hive_db_name}.cancellationreason')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f589b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some queries\n",
    "\n",
    "airport.printSchema()\n",
    "flight.printSchema()\n",
    "cancellationreason.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ccfa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT flightid, cancelled, cancellationcode FROM flight LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f65c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM airport LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM cancellationreason LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce77715",
   "metadata": {},
   "source": [
    "### Data prep for ML modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7648ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, sin, cos, pi, hour, minute, dayofmonth, dayofweek, year as f_year\n",
    "from pyspark.sql.types import IntegerType, StringType, DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_table_name = \"flight\"\n",
    "df_flights = spark.table(f\"{hive_db_name}.{hive_table_name}\")\n",
    "df_flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a180a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = df_flights.count()\n",
    "print(f\"Total number of rows: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619cf14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_flights.columns\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5748fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = df_flights.select([\n",
    "    F.count(F.when(F.col(c).isNull(), c)).alias(c)\n",
    "    for c in df_flights.columns\n",
    "])\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b021ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'flightid', 'year', 'month', 'dayofmonth', 'dayofweek',\n",
    "    'deptime', 'crsdeptime', 'arrtime', 'crsarrtime',   # timestamps (will be added transformed later)\n",
    "    'actualelapsedtime', 'crselapsedtime',\n",
    "    'airtime', 'arrdelay', 'depdelay',\n",
    "    'origin', 'dest',\n",
    "    'distance',\n",
    "    'taxiin', 'taxiout', 'diverted',\n",
    "    # 'carrierdelay', 'weatherdelay', 'nasdelay', 'securitydelay', 'lateaircraftdelay',   # a lot of nulls\n",
    "]\n",
    "\n",
    "time_features = ['deptime', 'crsdeptime', 'arrtime', 'crsarrtime']\n",
    "\n",
    "label = \"cancelled\"\n",
    "\n",
    "df_processed = df_flights.select(features + [label])\n",
    "# mb cast cancelled col to int\n",
    "df_processed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fba3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Handle Missing Values (simple drop for this example, consider imputation)\n",
    "df_processed_clean = df_processed.na.drop()\n",
    "print(f\"Number of rows after NA drop: {df_processed_clean.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Time/Date Feature Engineering\n",
    "def parse_time_hour_udf(time_str):\n",
    "    if time_str is None: return None\n",
    "    try: return int(time_str.split(':')[0])\n",
    "    except: return None\n",
    "\n",
    "def parse_time_minute_udf(time_str):\n",
    "    if time_str is None: return None\n",
    "    try: return int(time_str.split(':')[1])\n",
    "    except: return None\n",
    "\n",
    "udf_parse_hour = udf(parse_time_hour_udf, IntegerType())\n",
    "udf_parse_minute = udf(parse_time_minute_udf, IntegerType())\n",
    "\n",
    "df_processed = df_processed.withColumn(\"ScheduledDepHour\", udf_parse_hour(col(\"CRSDepTime\")))\n",
    "df_processed = df_processed.withColumn(\"ScheduledDepMinute\", udf_parse_minute(col(\"CRSDepTime\")))\n",
    "df_processed = df_processed.na.drop(subset=[\"ScheduledDepHour\", \"ScheduledDepMinute\"])\n",
    "\n",
    "df_processed = df_processed.withColumn(\"DepHour_sin\", sin(2 * pi() * col(\"ScheduledDepHour\") / 24.0))\n",
    "df_processed = df_processed.withColumn(\"DepHour_cos\", cos(2 * pi() * col(\"ScheduledDepHour\") / 24.0))\n",
    "df_processed = df_processed.withColumn(\"Month_sin\", sin(2 * pi() * col(\"Month\") / 12.0))\n",
    "df_processed = df_processed.withColumn(\"Month_cos\", cos(2 * pi() * col(\"Month\") / 12.0))\n",
    "df_processed = df_processed.withColumn(\"DayOfMonth_sin\", sin(2 * pi() * col(\"DayofMonth\") / 31.0)) # Approx.\n",
    "df_processed = df_processed.withColumn(\"DayOfMonth_cos\", cos(2 * pi() * col(\"DayofMonth\") / 31.0))\n",
    "df_processed = df_processed.withColumn(\"DayOfWeek_sin\", sin(2 * pi() * col(\"DayOfWeek\") / 7.0))\n",
    "df_processed = df_processed.withColumn(\"DayOfWeek_cos\", cos(2 * pi() * col(\"DayOfWeek\") / 7.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Categorical Feature Encoding\n",
    "categorical_cols = [\"Origin\", \"Dest\"]\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_index\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=indexer.getOutputCol()+\"_ohe\") for indexer in indexers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be0a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Numerical Feature Scaling\n",
    "numerical_cols_raw = [\"Distance\", \"CRSElapsedTime\", \"Year\"] # Year is not cyclical in the same way but can be scaled\n",
    "cyclical_cols_engineered = [\n",
    "    \"DepHour_sin\", \"DepHour_cos\", \"Month_sin\", \"Month_cos\",\n",
    "    \"DayOfMonth_sin\", \"DayOfMonth_cos\", \"DayOfWeek_sin\", \"DayOfWeek_cos\"\n",
    "]\n",
    "# Assemble numerical features for scaling\n",
    "temp_numerical_assembler_inputs = numerical_cols_raw + cyclical_cols_engineered\n",
    "temp_numerical_assembler = VectorAssembler(inputCols=temp_numerical_assembler_inputs, outputCol=\"temp_numerical_features\", handleInvalid=\"keep\")\n",
    "scaler = StandardScaler(inputCol=\"temp_numerical_features\", outputCol=\"scaled_numerical_features\", withStd=True, withMean=False) # Mean can be sensitive to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e863d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Assemble Final Features Vector\n",
    "final_assembler_input_cols = [encoder.getOutputCol() for encoder in encoders] + [\"scaled_numerical_features\"]\n",
    "vector_assembler = VectorAssembler(inputCols=final_assembler_input_cols, outputCol=\"features\", handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ffe7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Split Data ---\n",
    "(train_data, test_data) = df_processed.randomSplit([0.8, 0.2], seed=42)\n",
    "train_data.cache() # Cache for repeated use in CV\n",
    "test_data.cache()\n",
    "print(f\"Training data count: {train_data.count()}, Test data count: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab9b1b",
   "metadata": {},
   "source": [
    "### ML modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f79b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Models, Pipelines, and Tuning ---\n",
    "lr = LogisticRegression(labelCol=\"Cancelled\", featuresCol=\"features\")\n",
    "rf = RandomForestClassifier(labelCol=\"Cancelled\", featuresCol=\"features\", seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5906e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pre-processing stages (common for both models)\n",
    "preprocessing_stages = indexers + encoders + [temp_numerical_assembler, scaler, vector_assembler]\n",
    "\n",
    "pipeline_lr = Pipeline(stages=preprocessing_stages + [lr])\n",
    "pipeline_rf = Pipeline(stages=preprocessing_stages + [rf])\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Cancelled\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
    "\n",
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5]) \\\n",
    "    .build()\n",
    "\n",
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [20, 50]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "cv_lr = CrossValidator(estimator=pipeline_lr, estimatorParamMaps=paramGrid_lr, evaluator=evaluator, numFolds=3, parallelism=4, seed=42)\n",
    "cv_rf = CrossValidator(estimator=pipeline_rf, estimatorParamMaps=paramGrid_rf, evaluator=evaluator, numFolds=3, parallelism=4, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70548561",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Training Logistic Regression ---\")\n",
    "cv_model_lr = cv_lr.fit(train_data)\n",
    "best_model_lr = cv_model_lr.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f092023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Training Random Forest ---\")\n",
    "cv_model_rf = cv_rf.fit(train_data)\n",
    "best_model_rf = cv_model_rf.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate Models ---\n",
    "print(\"--- Evaluating Models on Test Data ---\")\n",
    "predictions_lr = best_model_lr.transform(test_data)\n",
    "predictions_rf = best_model_rf.transform(test_data)\n",
    "\n",
    "auc_pr_lr = evaluator.evaluate(predictions_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f99508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detailed_metrics(predictions_df, label_col=\"Cancelled\", pred_col=\"prediction\"):\n",
    "    \"\"\"Calculates and returns detailed classification metrics.\"\"\"\n",
    "    preds_and_labels = predictions_df.select(pred_col, label_col).rdd.map(lambda r: (float(r[0]), float(r[1])))\n",
    "    metrics = MulticlassMetrics(preds_and_labels)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        \"confusion_matrix\": metrics.confusionMatrix().toArray().tolist(),\n",
    "        \"precision_0\": metrics.precision(0.0),\n",
    "        \"recall_0\": metrics.recall(0.0),\n",
    "        \"f1_0\": metrics.fMeasure(0.0),\n",
    "        \"precision_1\": metrics.precision(1.0), # For Cancelled\n",
    "        \"recall_1\": metrics.recall(1.0),       # For Cancelled\n",
    "        \"f1_1\": metrics.fMeasure(1.0),         # For Cancelled\n",
    "        \"accuracy\": metrics.accuracy\n",
    "    }\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1a313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_metrics_lr = get_detailed_metrics(predictions_lr)\n",
    "print(f\"Logistic Regression - Test AreaUnderPR: {auc_pr_lr}\")\n",
    "print(f\"Logistic Regression - Test Precision (Cancelled): {detailed_metrics_lr['precision_1']}\")\n",
    "print(f\"Logistic Regression - Test Recall (Cancelled): {detailed_metrics_lr['recall_1']}\")\n",
    "print(f\"Logistic Regression - Test F1 (Cancelled): {detailed_metrics_lr['f1_1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcbb6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_pr_rf = evaluator.evaluate(predictions_rf)\n",
    "detailed_metrics_rf = get_detailed_metrics(predictions_rf)\n",
    "print(f\"Random Forest - Test AreaUnderPR: {auc_pr_rf}\")\n",
    "print(f\"Random Forest - Test Precision (Cancelled): {detailed_metrics_rf['precision_1']}\")\n",
    "print(f\"Random Forest - Test Recall (Cancelled): {detailed_metrics_rf['recall_1']}\")\n",
    "print(f\"Random Forest - Test F1 (Cancelled): {detailed_metrics_rf['f1_1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04083c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save Models and Predictions ---\n",
    "print(\"--- Saving Models and Outputs ---\")\n",
    "model1_path_hdfs = \"project/models/flight_cancellation_lr_model\"\n",
    "model2_path_hdfs = \"project/models/flight_cancellation_rf_model\"\n",
    "\n",
    "best_model_lr.write().overwrite().save(model1_path_hdfs)\n",
    "print(f\"Saved Logistic Regression model to: {model1_path_hdfs}\")\n",
    "best_model_rf.write().overwrite().save(model2_path_hdfs)\n",
    "print(f\"Saved Random Forest model to: {model2_path_hdfs}\")\n",
    "\n",
    "predictions_lr.select(\"Cancelled\", \"prediction\") \\\n",
    "    .coalesce(1).write.mode(\"overwrite\").format(\"csv\") \\\n",
    "    .option(\"header\", \"true\").save(\"project/output/model1_lr_predictions\")\n",
    "print(\"Saved LR predictions to project/output/model1_lr_predictions\")\n",
    "\n",
    "predictions_rf.select(\"Cancelled\", \"prediction\") \\\n",
    "    .coalesce(1).write.mode(\"overwrite\").format(\"csv\") \\\n",
    "    .option(\"header\", \"true\").save(\"project/output/model2_rf_predictions\")\n",
    "print(\"Saved RF predictions to project/output/model2_rf_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c77de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save Evaluation Comparison ---\n",
    "evaluation_summary_data = [\n",
    "    (\"Logistic Regression\", auc_pr_lr, detailed_metrics_lr['precision_1'], detailed_metrics_lr['recall_1'], detailed_metrics_lr['f1_1']),\n",
    "    (\"Random Forest\",       auc_pr_rf, detailed_metrics_rf['precision_1'], detailed_metrics_rf['recall_1'], detailed_metrics_rf['f1_1'])\n",
    "]\n",
    "eval_schema = [\"ModelName\", \"AreaUnderPR\", \"Precision_Cancelled\", \"Recall_Cancelled\", \"F1_Score_Cancelled\"]\n",
    "evaluation_df = spark.createDataFrame(evaluation_summary_data, schema=eval_schema)\n",
    "evaluation_df.show(truncate=False)\n",
    "evaluation_df.coalesce(1).write.mode(\"overwrite\").format(\"csv\") \\\n",
    "    .option(\"header\", \"true\").save(\"project/output/model_evaluation_comparison\")\n",
    "print(\"Saved evaluation comparison to project/output/model_evaluation_comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.unpersist()\n",
    "test_data.unpersist()\n",
    "spark.stop()\n",
    "print(\"--- Pipeline Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846a3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974cf607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
